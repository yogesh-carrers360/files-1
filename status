created new resource group using azure cli
then created aks cluster with one node using cli command "az aks create --resource-group myResourceGroup-yogesh --name myAKSCluster --node-count 1 --enable-addons monitoring --generate-ssh-keys" 
updated cube config file with this command for authentication "az aks get-credentials --resource-group myResourceGroup-yogesh --name myAKSCluster"
checked if i can communicate with akscluster with kubectl get pod -A 
created nginx pods using Yml file with the help of "kubectl explain" and deloyed pod using "kubectl create -f ymlfile".
then I have created one service "type=Loadbalancer" for expose ports" kubectl expose pod webserver --type=LoadBalancer --port=80 --name=example-service"
checked service created or not and checked external IP using this command "kubectl get svc"


I have explored for Kubernetes services, read about how to create yamls for services and how to expose ports using that with types NodePort, ClusterIP and LoadBalancer.
Then I have read about Replicacontroller , Replicaset and difference between them like equality base and Set base selectors.
I have created one Deployment.yaml file and deployed one sample NGINX image using that, then I have learned about Strategies like RollingUpdate, Recreate. Read about Deployment rollout using annotations, Resource limits based on CPU, RAM.
Then I have created one deployment file for mongodb once deployed then I have created one service.yaml file for exposing it, added content of service.yaml file in Deployment.yaml file itself. and deployed it and got IP for accessing mongodb.Then I have updated Backend service yaml file with mongodb IP and built docker image and pushed into dockerhub.
Then created another Deployment.yaml for backend service, and gave docker hub backend image in that file and deployed with service for backend, tried hitting backend IP, and got expected response then I have done same process for frontend and accessed it through its IP and I was able to see frontend page


created cluster applying terraform after getting access to gitlab repo to managek8s and gone through terraform code managek8s.
go to overview of istio from vardan and got files for deploying istio then teried to deploy istio manually in cluster.
once it got deployed then i have upadted terraform code and added steps to install deploy istio and added deployment files in mainfest folder.
Added "-y" in istioctl command for auto confirmation. then i ran terraform apply and once it got succecced when i have worked to add true or false in istio.
added istio depencies on kubeconfig so that its deployment not get started before creating cluster.
i have done changes for istio deployment in aws k8s automation when i applied terraform applied i have got errors of spot instances. so, I have changed into on demand for testing.


Firstly I have tried to resolve the output.tf issue which I was getting when any service will be set to false, tried to apply count in output.tf but the error still persists.
Explored for error and tried many solutions as provided in Blogs and stackoverflow and at last tried  this "var.sonarqube_enabled ? module.sonarqube[0].sonarqube_password : null" and error got resolved.
Worked with @Vasu Gupta for deployment of infra and services  part.
Cleaned up Singapore region.
Updated README.md file only for services part, screenshot attached.
Added lables in application node group.
and pushed updated code into aws/qa branch

I have explored for loki-grafana dashboard. then I have deployed grafana on eks cluster using terraform.
Then I have manually added nginx-ingress and loki dashboard by number and JSON file in grafana console.
Then I have added new yaml file for creating config map includes json of Loki dashboard in dashboard directory in helm charts for automatically deploying with automation, but when did terraform apply  it was not able to create config map for Loki dashboard, explored and tried some ways to rectify that. Then destroyed all services using terraform destroy and applied again but same problem still persists. There were 2 directories for yamls so moved that yaml into other directory and terraform applied again but problem was not solved.
Destroyed all services setup, and had a meeting with @Vardan Sharma  if he can go through or if the problem because of my server.
@Vardan Sharma  has tried to apply and was able to add loki dashboard automatically.
Then I have worked on adding resource limits for Loki, updated values file for that and applied.  


Firstly I have tried to resolve the datasource issue when adding dashboard, datasource was not adding in dashboard in grafana, explored for that and tried many solutions as provided in Blogs and stackoverflow then had meeting with @Vardan Sharma and changed name of datasource from DS_PROMOTHEUS to promotheus then the issue got resolved.
Then destroyed grafana service using terraform destroy and then I have added new yaml file for creating config map which includes json of nginx-ingress dashboard and ran terraform apply, then it shown error "function "ingress" not defined", tried to apply many solution which given by blogs at last I have tried this {{` {ingress} `}} instead of { { ingress }}  as written in one blog then the error got resolved and I was able to get nginx-ingress dashboard in grafana with datasource. Did same for host error  in json.
Pushed updated code in aws/dev branch, currently working on blackbox deployment.


I have explored for black-box exporter. Back-box exporter is a agent and monitors endpoints http, https and ssl expiry etc.
firstly I have deployed prometheus on cluster using terraform. then I have manually added blackbox-box exporter using JSON file but there were no data source.
Then i have followed this doc https://lapee79.github.io/en/article/monitoring-http-using-blackbox-exporter/. then I have created Blackbox configuration file as ConfigMap to configure http. then deployed blackbox exporter to kubernetes cluster, and tested blackbox exporter using portforward on 9115 port.
Then I have created Prometheus config file and define google.com endpoint and  facing issues when we create secret using cat command, then I have tried with kubectl --from file then I have created secret successfully.
Then I have edited service which was created from black-box exporter and defined spec secret name in spec: section.
and checked with grafana dashboard I was able to see blackbox dashboard populated from values coming from cluster.
Done this manually and currently working on added on automation with helm chart.

Firstly I have tried to deploy blackbox_expoter in automation with helm.
I have added helm_release resource for blackbox_exporter in prometheus-grafana main.tf and also added values.yaml in helm/vlues directory for blackbox_exporter.
we have define node_affinity and resource limit in values.yml for blackbox_exporter then I have changes in prometheus values.yaml file. I have defined services name of blackbox_exporter and google.com endpoint in additionalScrapeConfigs section.
Then I have added new yaml file for creating config map includes json of blackbox_exporter in dashboard directory in helm charts for automatically deploying with automation.but when did terraform apply it was not able to create config map for blackbox_exporter dashboard, explored and tried some ways to rectify that. then ran terraform apply then it shown error of configmap name then I have remove "_" from balckbox expoter and again ran terraform apply then it shown error "function "target" not defined" then I have tried this {{` {target} `}} instead of { { target }}  as written in one blog then the error got resolved.
Then ran terraform apply and deployed successfully, I have checked with grafana dashboard then it show the dashboard of blackbox_exporter and set the data for last 1 hour and see the data of 1 hour and also see status http status code and ssl, ssl expiry.
Then I have tested with dockerfile which was already created by Siddharth and build dockerimage from that dockerfile.
Then I have ran "docker run" command then  failed to start docker container. then I have defined sleep infinity in dockerfile,  ran "docker run" command and exec into the container and checked if requried things(like kubectl,terraform etc..) are installed or not.
Added COPY command in dockerfile for having source file of terraform code inside docker conatiner, ran terraform apply in terraform source directory then it show error permission denied for creating .terraform directory, then I have set root user in dockerfile. and then ran terraform apply and it works fine.
Pushed code in aws/dev branch and currently working on README.md for eks part.   

Firstly, I have upgraded terraform version from v0.14.0 to v1.0.0. and pulled latest code from aws/qa branch.
Then I have cd into source directory of managed-k8s and ran terraform init, it ran successfully.
After that I ran command terraform validate it showed error "failed: the "list" function was deprecated in Terraform v0.12 and is no longer available; use tolist([ ... ]) syntax to write aliteral list", I have commented out output.tf content still I have got same error. tried after removing output.tf but error not resolved.
Tried after removing .terraform but error still persist.
Explored for that and tried many solutions as provided in Blogs and stackoverflow tried list[""] inated of list()in variable.tf but error not resolved.
I have used tolist([string]) instead of list(string) but error still persist .I have tried with element and changes with brackets and other solution as provided in the blog but error not resolved.
It's  running fine with terraform version 0.14.0 and But when I used v 1.0.0. i have got this error and currently working on this.


Worked on this error: "failed: the "list" function was deprecated in Terraform v0.12 and is no longer available; use tolist([ ... ]) syntax to write aliteral list".
Firstly I have explored about terraform upgrade command and tried to ran terraform 1.0.0upgrade command it show "terraform has no command". and check with terraform -h there is no option for upgrade.
Then I have tried terraform fmt for testing purpose but error not resolved.
After that I have used this tolist("") instead of list([""]) in .terraform/module/eks/output.tf and then error got resolved on terraform validate. then I ran terraform plan it show error in .terraform/module/eks/locals.tf, so Id have remove .terraform and other directories.
I ran terraform init i have seen terraform-aws-modules/eks/aws 14.0.0 explore for version 1.0.0 then I have changed the terraform-aws-modules/eks/aws 17.1.0 instead of 14.0.0 ran terraform init then it show error Failed to query available provider packages and tried required_version = ">= 1.0.0" in version.tf. but error not resolved.
Then  used aws version 3.43.0 instead of 3.29.0 and ran terraform init. and it ran successfully. then ran terraform validate and terraform plan it ran successfully.
 Ran terraform apply it created all stack successfully and after that ran terraform destroy. and it destored all stack successfully. terraform destroy working  with terraform version 1.0.0 @anushka
I have used  eks module version 17.1.0 instead of 14.0.0 in eks-cluster.tf and use aws version 3.43.0 instead of 3.29.0. and it worked fine.
Currently working for services part.


Firstly, I have tried to deploy services in eks cluster.
Ran terraform init and terraform validate it ran successfully.
Ran terraform apply it show error " If you do intend to export this data, annotate the output value as sensitive by adding the following argument:
│     sensitive = true" in output.tf then i have used sensitive = true I have not got error but it didn't showed endpoint and password, I tried sensitive=false but it show error.
I have Explored for error and tried many solutions as provided in Blogs and stackoverflow and at last tried nonsensitive in output.tf but it not work for main ouput.tf. then I have defined nonsensitive in particular service's main.tf in services directory. and then it woked fine.
Connected with Aasish he told about kube-scan-lb.yaml and it creates NLB.
 Explored about kube-scan and got to know that it analyzes security settings and gives you a security score of workload.
I have applied kube-scan-lb.yaml and it creaets NLB checked services for that . Then hit the loadbalancer endpoint on browser and it show security score in web ui (like 7 is high risk, 5 & 6 medium risk). and currently working on load-testing on eks-aws.


Firstly, I have worked to update previous work and then updated Readme.md file.
After that I have shared Readme link  with Vasu to review.
Explored for  Portainer as "Simple management UI for Docker", installed portainer with manifest file and then i have tried with helm chart.
And hit LoadBalancer endpoint with 9000 port and logged into dashboard of protainer.
As per I tested:-
I have seen that portainer dashboard UI slow compare to rancher dashboard.
Monitoring feature available in rancher  as well portainer dashboard
Viewing  logs of pods and exec into pod in rancher as well portainer.
Rollback feature available in portainer as well as rancher.
Authentication methods internal Oauth in portainer as well as in rancher.
Catalog added like helm is not in portiner. and information about HPA in rancher but not portainer.
Added registry of docker in portainer not available in rancher.
Took feedback from vasu and will update tfvars file name from reference.tfvars to terraform.tfvars.
Main Features of Portainer are:
Docker management
Docker UI
Docker cluster management

Firstly I have explored how to decrease risk of Kubernetes services which was shown by Kube-scan, it shows the Vulnerability of Prometheus-operator-Prometheus-node-exporter that is  performing as  Man-in-the-Middle,  then I have Updated README.md of service part.
After that worked on loki-distributed and follow this link https://github.com/grafana/helm-charts/blob/main/charts/loki-distributed and clone repo in local and then run helm install command. deployed helm chart successfully but there two pods are running and rest of the pods are show error. Then I have checked the logs of the pods then it showed error "level=error ts=2021-06-28T12:34:17.688459139Z caller=log.go:106 msg="error running loki" err="failed services\ngithub.com/grafana/loki/pkg/loki.(*Loki).Run\n\t/src/loki/pkg/loki/loki.go". I have tried with defining the storage class in values.yaml, I  have explored for that but error still persist, and currently working on loki-distributed helm chart.
https://gitlab.com/squareops/k8s-services/-/blob/dev/README.md @nitin sir please review that.


Firstly I have defined default storage class in services part, then I have explored for namespace based node-affinity.
Tested namespace based node-affinity and deployed microservices to test, out of 10 microservices 2 have deployed into other node, when I tested with 2 microservices both have deployed into desired nodes and explored for that but issue is pending. 
Then,I have explored for HA Setup, first of all I have checked with nodes that nodes are multi az or not by increasing the size of nodes in auto scaling group and checked that they were made in multi az.
After  that I have tried to deploy application in multi-az with pods and defined pod affinity in deployment files. then increases the replicas and pods scheduling per az, but problem is that when increases the replicas set to 4 there is no pods scheduling & pod status is pending because there is no az left, so I have  explored for that and tried many solutions as provided in Blogs and stackoverflow at last I have tried - weight: 100 podAffinityTerm: , and the pending pod was able to schedule after that.


Firstly I have tried to deploy all microservices for multi-AZ HA  pod deployment by defining annotation in deployment file manually and tested that, then I have tried to pass annotation in deployment file with sed command and done that with sed command but need to write line number manually in sed command where annotation needs to be updated.
As suggested by  Nitin sir, I have explored for the kubectl patch command  and I have deployed sample application which was specified in doc, and defined patch in other yaml file. then I ran command "kubectl patch deployment patch-demo --patch "$(cat patch-file.yaml)", then I have checked if annotations are updated or not in deployment file using kubectl edit command.
It works for only single deployment so I have explored for that and tried many solutions as provided in Blogs and stackoverflow then I have thought about updating all deployment of a namespace so I  have created  this command " kubectl get deployments -o name | sed -e 's/.*\///g' | xargs -I {} kubectl patch deployment {} --type=json -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/imagePullPolicy", "value": "Always"}]'"" and then modified for file "kubectl get deployments -o name -n sock-shop | sed -e 's/.*\///g' | xargs -I {} kubectl patch deployment {} --patch "$(cat patch-file.yaml)" -n sock-shop" and then created shell script for that and variabilize the namespace for this command, and currently working on update rds endpoint for microservices.  

I have created RDS using terraform automation code.
I have tried to Update the RDS Endpoint and username, password, but we are not having application code in that repo, only deployment files and docker images are there.
As suggested by  Nitin sir, i have forked the repo of microservices in our gitlab repo then  I have cloned the code of carts service and checked that how to pass database credential in application code. They have passed "MYSQL_PORT_3306_TCP_ADDR" hostname and password like this, but when I have passed rds endpoint with env. variable in deployment file, it not worked.
After that I have made env variable according to the code and passed in deployment file update username and passwords as well as RDS Endpoint.
Checked  logs of pods and  describe the pod but no error log was there,  tried to build image using provided dockerfile in source code. then I have got error "package not found" and currently working on that.

Firstly I have tested with deployed catalogue service as provided in application repo and then checked with env variable in catalogue pod with echo command but there were no values. then I have checked  in their code and there is docker-compose file they have pass variable in that file.
Then tried to deploy application with the help of helm charts, got this error "but missing in charts/ directory: nginx-ingress". this error came from  requirements.yaml  so, I have removed that file and after that it was able to deploy.
After that I have enabled the load-test in values.yaml of microservices helm charts then ran helm upgrade command for that and checked with pods, there are two pods of load-test there is only one pod which is up and another is down due to imagePullbackoff error.
Had meeting with Anish sir, tested that app is not using the db because, we did not find any variable for db in docker-compose or dockerfile plus there is a .json file. Also shutting down the db pod did not affect the app. He suggested look for other app which has a direct dependency on db.
I have tried to changed elastic ip of NAT gateway but not able to change elastic ip of NAT gateway due to some error of permission as I have posted in tech-discussion, after getting access  also I was not able to do that.
Explored about locust and  installed in the system, then I have created sample file for locust locust.py and ran command locust -f locust.py and I was able to see the GUI face of the locust at port number 8089. 

Firstly, Had meeting with vardan discussed about the tasks which I have to do.
Need to integrate true and false for creation of GPU Nodepool. Convert terraform binaries to work with v1.0. Check conditions based on kubenet and azure cni networking for aks cluster.these are task assigned by vardan. posted for access on repo and as well as create user for azure.
Then I have started to work on Namespace based scheduling and follow the link which has been shared by vardan https://github.com/idgenchev/namespace-node-affinity.
I have deployed all yaml files which is under the deployment/base directory . facing issues while deploying deployment like "not defined selector" etc.
After that created a namespace manually, To enable the namespace-node-affinity use this command "kubectl label ns Infra namespace-node-affinity=enabled", then I have defined nodeselector in examples/sample_configmap.yaml and applied that, deployed sample microsservices application in that namesapce, and checked with command "kubectl get pods -o wide" and out of 10 microservices 2 have deployed into other node, and issue not resolved till now.
I have gone through repo which was shared by rohit sir and check with shipping services it is based on java spring boot, then checked with shipping directory of that repo and passed rds endpoint, username & password as well.  I have tried to  create Docker image and I was able to create docker image and successfully passed rds endpoint. after that  successfully passed with env variable only host name with this command "docker run -e DB_HOST=database-2.cnbwakbdzeez.us-east-2.rds.amazonaws.com ship".
Then worked with vasu  & deployed all helm chart through terraform automation and  values.yaml variablising pending  and currently working on that. 

Firstly I have destroyed old cluster because I was not able to  access old cluster and deployed new cluster using terraform automation, then I have tried to resolve conditional based gpu node scheduling, I have tried with this  for_each   = var.enable_gpu ? var.node_pool : var.gpu_node as used in count  but got error and tried with that line under dynamic resource as followed  in this doc https://codeinthehole.com/tips/conditional-nested-blocks-in-terraform/ and tried with for_each   = var.enable_gpu ? [var.node_pool] : [var.gpu_node] and tried with list and tolist but error still persist.
I have tried with blog for the namepace based schduling https://stackoverflow.com/questions/61407958/dedicated-nodes-for-namespace and other is https://github.com/idgenchev/namespace-node-affinity. but  not able to implement namesapce based affinity.
After that, I have tried with sample application tried to pass mongodb url with catalogue application. Install mongodb in local and pass loacalhost mongo endpoint in application code. and checked with npm install-> node server.js . I have created  documentdb on aws, first of all I have tried to connect documentdb with loacal machine and I have got error then  checked with subnet it make public subnet or private subnet. then I have checked that it make private subnet, then I will check tomorrow with create instance with same vpc of documnetdb.


Firstly, I have created new documentdb and new instance in same vpc and tried to connect documentdb with instance and I have successfully connected with database using mongo client after that I have tried to pass documentdb endpoint in application but i have got error "ERROR {\"name\":\"MongoNetworkError\"}","v":1}".explored for that i thought this error due to ssl certificate didn't pass properly, explored for how to pass documentdb endpoint with ssl but error still persist and posted error in group as well.
Had meeting with nitin sir, he asked for deploying new cluster with bigger nodegroups and double the size and Install rancher,grafana.opendistro as well then i have changed the configuration of tfvars and deployed eks cluster and services as well as. then i have faced issue in deploying opendistro due to storage class then I have resolved that issue.
After that I have deployed sample application of gcp with change service type loadbalancer to ClusterIP and for applying ssl on sample application create a ingress file defined secret, hosts & annotation of lets'encrypt, then deployed that and then applied hpa on sample application, nitin sir gave feedback about grafana "blackbox exporter does not have the application URL configured by default, Nginx ingress dashboard has no data to display" then i have done changes with promotheus values.yaml file with url of sample application and then update configmap of blackbox-exporter in helm chart, ran again terraform apply and issue got resloved.
Then I have asked vardan for help in namesapce based node affinity but he has not got time today.
Tried to pass documentdb endpoint in sample appliction and then tried with this sslCA: [fs.readFileSync("rds-combined-ca-bundle.pem")] and connection success
Had meeting with vasu and anish issue got resolved of rancher issue related to update deployment with config, he asked for appling helm chart locally and images should not come from public repo and push into ecr provite repo.

Firstly, Had meeting with vasu and he asked for applying helm chart manually for rancher, sonarqube and grafana and also images should not come from public repo and push into ecr private repo, then i have posted estimate time for that in group.
Then i have created a new rds and documentdb in same vpc of cluster and  passed the database endpoint in application, built docker image from that code and pushed image into ECR, defined ECR endpoint in deployment file of services, then ran helm install command and all but there is redis pods not came up then i have checked with logs of pods and the error of storage class and updated storage class name in values.yaml file again ran helm install then all pods are running up and hit endpoint of application. when checked with login into application it shows error "ERROR database not available".
After that I have created new ec2 instance and login with documentdb created a new databases which name mydb. and update into deployment file and ran helm upgrade command. checked again with login but error still persist.
I have deployed sample application as provided in repo in another namespace and checked with login. I logged into successfully, then i have created new databases in documentdb which name is users and build images again and pushed into ecr and then update in deployment and checked  again with login but error still persist.
I have checked with exec into pods of users where application images are came from officially checked that thay have created collection the mongodb,  explored for that and created a new collection in documentdb and update into code and build image again and update latest image into deployment file then i have checked with login but error still persist. currently working on that.


Firstly, I have created database and collection in documentdb with same as provided in the application code and  built docker image & pushed into ECR, Updated  deployment file of helm chart and applied helm install command, checked with pods logs and exec into pods,  hit application url checked with login and successfully logged into the application.
Then I have Explored about Jenkins, how to install plugins and how to create pipeline etc.. in jenkins.
Had meeting with Rohit sir, Took overview of the jenkins how to make connection in jenkins with gitlab, created personal access token in gitlab, then gone through jenkins ui interface. Gone to manage plugin and install gitlab plugin,then created a new project and choose pipeline option and configuration of the project. when i have defined URL of gitlab repo it not shown the API token option in drop-down menu.then I have tried with username & password but error still persist. Explore for that and tried to resolve that but error not resolved.
After that, posted error into group and had meeting with Rohit sir, he tried to connect gitlab with system configuration available in manage jenkins menu. But error still persist, then he tried with sshkey, generated public & private ssh-key and public key defined in gitlab repo and private key in jenkins credential and error got resolved.
After that, I have uploaded jenkinsfile in gitlab repo which was shared by nitin sir, click on build now option it show error then explore for that and install plugin of docker and Amazon ecr plugin. then trigger build in jenkins checked that there are two pods are running up for build docker images. then it show error of "java.lang.NoSuchMethodError: No such DSL method 'withAWS' found among steps" currently working on that.
Destroyed cluster that was deployed yesterday.


Firstly, I have Explored how to pass AWS credential in jenkins then I have Installed AWS Plugin,  AWS ECR plugin and pipeline plugin, I have passed the aws credential in jenkin in manage jenkins menu.
After that i have Deployed all microservices in new namespace and tried to update images in deployment, then I have created Jenkinsfile only for catalogue service, created a public repo in ECR and defined url of ECR in jenkinsfile,  ran jenkins pipeline It showed error for path of Dockerfile,  changed the path of Dockerfile according to the repo as well as changed path of dockerfile content and ran  jenkins pipeline again. But It show docker login succeeded but when we ran docker push command it shows error as posted in group. I have explored for that and then I have checked that it have used docker push command of ecr private repo in jenkinsfile  created a new private ECR repo and ran it successfully and pushed imaged into ECR repo. Also checked there some pod are running for CI part.
Then I have defined the cluster name in jenkinsfile as made variables in jenkinsfile,  ran again jenkins pipeline it successfully updated kubeconfig file path.
Then I have explored that how to update image in deployment file in pipelines instead of helm upgrade, found a kubectl command which have update in deployment
"kubectl --record deployment.apps/catalogue set image deployment.v1.apps/catalogue catalogue=421320058418.dkr.ecr.us-east-2.amazonaws.com/jenks:${BUILD_NUMBER} -n roboo" then I have tried to install kubectl in CI pod using curl command but it show error of "sudo not use" in pipeline then  removed sudo from the command and ran again. It ran successfully and updated image in deployment.
Also I have noticed that During Update Image it create new pod then it terminate old pod. and it ran successfully for one services.
After that I have Posted jenkinsfile in group and created jenkinsfile for other services with update dokcerfile path, then created different pipelines for different services and ran pipelines issues some isues but resloved that and created pipeline successfully.
 worked on import existing databases in documentdb as told rohit sir in Scrum call. I have explored for that and gone through youtube video which was shared by rohit sir.
When I have successfully Import existing databses in documentdb and hit url application url browser It shown purchases robot and add to cart robot etc..
currently working on how to make default dashboard in grafana.
And here Application endpoint: a7cf48a1e94304b2e93fee63990dd8f1-231982658.us-east-2.elb.amazonaws.com:8080

Firstly, I have Explored about how to make dashboard as home page in grafana then i found link https://stackoverflow.com/questions/48164754/how-to-set-a-dashboards-on-grafana-home-page and followed that and successfully added dashboards on Grafana home page.
Then I have setup CI/CD pipeline for demo, then gave demo to Nitin and Ankush Sir. And took feedback from them, they asked to use custom images for jenkins CI that image contain packages  pre installed for build like kubectl, aws cli etc... and forked each microsercies in different repos, used Enviroment variable in CI/CD pipelines.
Then I have worked to forked each microsercies in different repository. but it show error while pushed code into repo then i have teried reslov  that but i was not able to push, i have posted error in the group.
Had meeting with vardan he gave overview of task which was assigned on gitlab task list. and task list are Conditional creation of GPU Nodepool, Grafana dashboard for monitoring GPU nodes,Deployment of Promtail as sidecar in pod.
After that i have worked to update create custom image that contain packages pre installed for build like kubectl, aws cli etc... and created a Dockerfile for that. I have tried to build image it show error "Error response from daemon: Dockerfile parse error line 1: unknown instruction: #" then i have explored and tried many as provided in stackoverflow and other blog at last i have found this link https://stackoverflow.com/questions/49088933/error-response-from-daemon-dockerfile-parse-error-line-1-unknown-instruction and error got resloved.
Then i have successfully created docker images and Pushed in Amazon ECR. and defined image path in Jenkinsfile.

Firstly, Had meeting with Nitin sir, He asked to Deploy sample application using configuration from environment variables and not hard coded. Identify all configuration items and create a configmap for these first and Start with one service.
Then i have explored for that how to create configmap. i have found kubectl command to create configmap then i have created config-map using that command, i have defined configmapkey in deployemt file. firstly i have passed wrong value of mongodb uri in configmap and checked with logs of pods it show error mongo connection,  i have passed right value of configmap and checked with pod logs and it show mongo connected.
After that configmap file shared by nitin sir and told that how to use configmap in deployment file. In that configmap file we can pass multiple values instead of passing multiples keys, then I have created a configmap using that file and i have successfully passed env variables in deployment file and checked with pod logs and mongodb connected successfully.
I have pulled catalogue services code and update configmap file in repo and created configmap in cluster using file which was added in catalogue-service repo and updated jenkinsfile with custom image which contain packages pre installed for build like kubectl, aws cli etc. and pushed code into new branch dev.
After that, I ran jenkins Pipeline and it ran successfully, updated latest images, passed env variables in deployment.

Firstly, When I hit jenkins endpoint on browser, endpoint not working and then I have checked with route53 and ingress are mapped with squareops.xyz, then I have mapped ingress with squareops.co.in and again applied ingress.yaml file of jenkins and error got resloved.
I have worked to configure configmap for all services, and created configmap file for particular services and defined configmap in deployemnt file and pushed code into dev branch of repo. and also configured the CI/CD pipelines of all services and tested that, its working fine.
After that, I have created secret in kubernetes, created secret using yaml and defined secret in deployment file. It configured only for one service cart and tested that it's working fine, once review ll be done by nitin sir then move forward with other services. as well as i have posted in group how i created secrets.
Then I have worked on Azure Automation, checked that Azure automation at par with standards used in Aws Automation and there is no automation for creating databases after disscuss with nitn and ankush sir, will start work on that.Then I have tried to resolve conditional based gpu node scheduling, passed condition in for_each as we use in the Count but it gave error, explored for how passed condtion in for each, i have tried many solution but error still persist.

I have Explored about how to create Sub-helm charts and create a master helm chart for that. then I have created charts directory and ran command "helm create subchart", created a configmap file in subchart/template and tried to passed data value using values.yaml file,subchart is a stand-alone chart change with values.yaml in subchart and checked with this command "helm install --generate-name --dry-run --debug subchart" and variables passed successfully with values.yaml of subcharts.
After That, Had meeting with Vardan, give overview of how to create multiple subchart in parent helm chart and also asked to tried with Overrided Values from a Parent Chart and global charts values.
Then I have tried with Overriding Values from a Parent Chart, then created a values.yaml file in parent chart and passed different value from subchart Values.yaml file then tested it. But it gave error "Error: Chart.yaml file is missing" then i have created Chart.yaml file in Parent Chart and tested again then it ran successfully.
After that I have tried with Global Chart Values created, used this {{ .Values.global.test }} in ConfigMap file and passed global: and put this value in values.yaml file and tested that it ran successfully and passed values globally.
I have tried to created sub-helm-chart for one microservice catalogue and created charts.yaml file for that. and created templates dir and upload deployment file and service and configmap file and created Values.yaml file for that. Also created parent chart for that. tested it but it gave error "Error: found in Chart.yaml, but missing in charts/ directory: catalogue", I have already created Chart.yaml but it gave error. I have explored for that found solution need to created requirements.lock, tried with this but error still persist.

Firstly, I have worked on new configuration for Web Project, the routing to backend services will be shifted from web nginx configuration to ingress. I have created ingress file for that and defined backend service path in ingress file and applied that file. but it gave error of 502, tried to resolved that, i have used this annotation "nginx.ingress.kubernetes.io/rewrite-target: /" frontend page working fine but it not connected with backend.
Then I have tried with worked on helm charts, Updated all the enviroment variable in configmap of helm charts and tested that but it gave error "{"apiVersion":"v1","data":{"CART_SERVER_PORT":8080,"CATALOGUE_HOST":"catalogue","REDIS_HOST":"red|...", i have explored for that and i have tried with "{{ .Values.catalogue_port }}" instead of {{ .Values.catalogue_port }} and error got resloved.
After that I have worked on Update Java project(Shipping) to include DB user and password as variables. then i have used this "String JDBC_USERNAME = String.format("%s", System.getenv("DB_USER") == null ? "admin" : System.getenv("DB_USER"));" for user and I have successfully passed username with ENV variable and I have used same thing for password and its working fine and currently working on ingress configuration for frontend service(WEB).   


I have worked on frontend (Web Project), the routing to backend services will be shifted from web nginx configuration to ingress, I have created two ingress file for that one for frontend and another backend apis and applied that but it gave error and also checked with exec into pod and ran command curl localhost/api/catalogue/products but it show error of connot GET /.
Had meeting with Anish sir, he asked to used this "nginx.ingress.kubernetes.io/rewrite-target: /$2/" and use this regex (/|$)(.*) and applied both file and issue got isolved. 
Then, Had meeting with nitin sir, reviwed ingress file and helm charts and he asked to add ingress file in helm charts and make proper jenkins CI/CD pipeline with single enviroment then will work on multiple enviroment like(prod, dev,stage).
After that, I have update the default.conf file of nginx then i have worked on adding ingress file in helm charts and adding hostname for service in values.yaml file and applied helm charts and applied successfully.
Then I have worked on jenkins CI/CD pipeline and created jenkins pipeline for each service with single enviroment and ran pipeline & ran successfully. then I have worked on rating service, it have some issues with rating service we can't able to rate the products. then i have checked path of api where to hit location path with inspect element in network section it show error 404. then I have exec into rating pod and checked with curl command with defining the path of api but it show 404, I have deployed their application from their repo in separate namespace and checked with rating service but rating is not working in their application also and currently working on Jenkins CI/CD with multiple enviroment.


Firstly, I have worked on Jenkins CI/CD pipeline, i have explored how to defined multiple stages in Jenkins file, i have created two different Jobs BUILD and DEPLOY in Jenkins pipelines and created for one service and tested that it works fine then i have defined multiple jobs for each service.
Then I have integrate GitLab Webbooks with Jenkins and give permission of push event for that, worked for each service in their particular repository, then gave demo to Nitin and Ankush Sir.
After that,I have explored about the vault integration for configmaps and secrets and i have followed this blog https://learn.hashicorp.com/tutorials/vault/kubernetes-external-vault?in=vault/kubernetes#install-the-vault-helm-chart which shared by nitin sir. then i have created account on hashicorp and choose free trial option and create vault cluster on hashicorp and pricing of vault cluster is 0.003/hr, created private vault cluster & provided private url of vault cluster in values.yaml file for that, passed values.yaml with helm install command, generated a token then ran helm install command & all pods are running up when checked with auth enabled command but it gave error of  no such Host, explore for that and tried many ways but error still persist. then I have created cluster again with publically and  it authenticate vault cluster successfully, Currently working on that. 

Firstly,I have explored about how to pass env variable container then i found this blog https://docs.docker.com/compose/environment-variables/ which uses process.env.NODE_ENV and i have checked with applocation code they are using same thing.
After that,I have crteated .env file and defined the ENV variable value in .env file then checked with application but it show error of mongo not connected, Explored for that then found export command for that "export $(cat /etc/environment | grep -v "#" | xargs)" after using this command enviroment variable passed successfully, then i have modified Dockerfile of application and used export command with ENTRYPOINT but it not working then i have explored for that and i have tried with this "CMD export $(cat .env | grep -v "#" | xargs) && node server.js" and pushed code into repo pipeline triggered & ENV variable passed through .env file.
Then I have explored about how to env variable from .env file without using export command. tried many solutions provided in stackoverflow and blogs and i have took idea from that we can used dotenv modules in our application. i have explored for that and found a blog, tried with npm install dotenv and used this require('dotenv').config() in our application, checked with application it successfully passed env variable from .env file without using export command.
Then I have modified dockerfile and added command npm install dotenv and pushed code into repo after build image successfully then i have checked with logs of the pod & env variable passed successfully in pod.
After that, I have checked for other languages(like python,go,java etc..) how to use dotenv modules in their particular languages application code, i have tried with springboot java language, created .env file for java application and tried env variable but not passed successfully and i have explored for that and tried with this spring.config.import=optional:file:.env[.properties] and added this line in application.properties but not passed successfully. 
Had meeting with nitin sir, he asked to pass env values from mounting configmap and then pass through vault cluster. then i have explored how to created configmap from file. then i have created configmap from file and Currently working on that. 


I have followed this blog https://learn.hashicorp.com/tutorials/cloud/vault-eks, firstly i have created vault cluster in hashicorp account.tried to install vault agents on EKS,i have generated a token and export into the Cluster as well as export endpoint of vault cluster. then added hashicorp helm repo using "helm repo add" command and created values.yaml file and inject	vault cluster endpoint in values.yaml file. then install vault using helm install command and passed values.yaml file with helm command.
After that, configured kubernetes auth method on HCP Vault using vault auth enable command, then configured Kubernetes service account, and get kubernetes certificate authority for the service account and cluster endpoint, configure the vault kubernetes auth method to use the service account token using vault write command, after configured authentication method of vault cluster then i have deployed their sample application example. first deployed databases on kubernates and added database role to vault using vault command.then i have configured Vault policy for database credentials.and deployed their application and checked with port forwarding and application are running fine.
Then i have checked env variable passed with exec into the application pods and also checked that vault pod is running with application pod.
After that i have same worked for own application, first i have configured kubernetes auth method on HCP Vault using vault auth enable command. when i have ran command "enable secret nginx" then it gave error of "plugin not found in the catalog: nginx" i have explored for that then i found we can build own plugin or we can used existing plugin in search engines of vault. then i have used existing plugin which name is kv and defined own secret in kv plugin,
created role and policy for that and also dfined this annotation-> "vault.hashicorp.com/agent-inject: "true"" in deployment file of our application deployed our application but it not running successfully and vault pods are not running up, i have tried with same again but error still persist currently working on that.  

Firstly, I have worked on pervious error of "authentication error" while doing vault integration for configmaps and secrets,when i have ran command "enable secret" then it gave error of "plugin not found in the catalog" i have explored for that then i found we can build own plugin or we can used existing plugin in search engines of vault. then i have used existing plugin from secret engine.
Then I have explored about how to create own roles, found this command for creating "vault write auth/kubernetes/role/app-role", and created own roles for our application and created a policy and gave read permission for that and defined .hcl extension for the file of Policy and save that ran command "vault policy write catalogue-policy catalogue-policy.hcl" and policy was created for that.
After that i have enabled secrets using this command "ault secrets enable -path=secret/ kv" and put secret into file using this command "vault kv put secret/catalogue-secret/catalogue" and passed MONGO_URL, defined vault annotation in deployment file of catalogue then I have applied deployment file of microservice but vault container not up it gave error of authentication error as well posted into group.
Then I have explored how to install helm charts using rancher UI, found option of catalogs in rancher Ui added helm repo in catalogs but it gave error of index,yaml file. then i have explored how to create helm repo and I found this blog https://tech.paulcz.net/blog/creating-a-helm-chart-monorepo-part-1/ and currently working on that.
Had meeting with vardan, told about the error of vault and he asked	for create a service account for that and i have explored for that and and defined serviceaccount in deployment file and applied that and it working fine.then I have got secret values from HCP Vault in my container at /vault/secrets/name location.
After that i have worked on how to use these secrets values with our application and used this command "node -r dotenv/config server.js dotenv_config_path=/vault/secrets/catalogue" in dockerfile and running fine.then I have explored about how to use env variable directly with are application
and found a blog and tried that i have facing some issue currently working on that.  

Firstly, I have explored about how to use env variable directly with are application or pods, found a blog and followed that,i have used vault annotation in deployment file of one microservice, used export with env variable.
Then I have used args in deployment file of microservice, first explored about how to export env values from file and used this command "['sh', '-c', 'export $(cat /vault/secrets/catalogue  | xargs) && node /opt/server/server.js']" and its working fine and passed env variable successfully.
After that, i have explored how to create secrets and configmaps from values stored in vault, and used vault annotation of ConfigMap in deployment file, created a configmaps file and defined role of vault, config path and env variable, public address of the vault. and applied both deployment and configmap file but pods are not running come up it gave error of crashbackloopff tried to resloved that but error still persist.
Had meeting with nitin sir, I have told about we have to export variable again and again for changing the values of secrets, he asked about UI interface of vault, we can configured secrets with UI interface of hashicorp vault.Also asked about used multiple enviroment variable within secrets and used CMD in deployment file instead of args commands cuerrently working on that.


firstly, I have tried with existing grafana deployed in cluster for showing logs according to pods of loki, then i have checked with loki dashboard there was no logs in loki and gave error of storage is full, i have tried to edit with pvc and increases the size of pvc 1Gi to 5 Gi and saved that but it not works.
After that, i have destroyed grafana from the cluster and deployed again and checked loki dashboard, it shows logs when selected of all pods.then i have explored how to displayed when we select a particular pod, then i have run this query {namespace="$namespace", pod=~"$pod"} |~ "$search"} instead of {namespace="$namespace", instance=~"$pod"} |~ "$search"} it display the logs when selected a particular pods.
Then I have worked on how added in automation, when we added save these query with changes it provides JASON file to create new dashboard.I  have added new yaml file for creating config map includes json(created JSON from that query) of Loki dashboard in dashboard directory in helm charts for automatically deploying with automation.
After that I have explored deployed HCP vault using terraform module, Then I have found this blog https://learn.hashicorp.com/tutorials/cloud/terraform-hcp-provider-vault and working on that.

Firstly, I have tried with this blog  https://aws-quickstart.s3.amazonaws.com/quickstart-hashicorp-vault/doc/hashicorp-vault-on-the-aws-cloud.pdf.They are autoscaling and LoadBalancer with for the vault-cluster, i have tried with that but it gave error.
Then I have explored how to install HCP-vault on ec2 server,then i found solution, tried with that choose ami from AWS market place and launch the EC2 server from that and open 8200 port number in security group, Hit Public IP on browser with port number but it gave error of Client sent an HTTP request to an HTTPS server. it required ssl certificate then i have explored that how to run without ssl certificate. then i have edited this file /etc/vault.d/vault.hcl and added tls-disabled=1 and restart the vault service & tried that and it working fine.
I have created autoscaling group and defined in LoadBalancer, But it gave error of unhealthy instance and i have tried to reslov that by changing of success code, health path and resolved that.
After that, I have explored how to install vault in ec2 instance and create custom image from that. using custom ami instead of AWS market place ami for vault. Currently working on that.


I have Explored about how to install vault in ec2 instance.downloaded the vault binary for linux and move into /usr/bin/ for make vault executable.then i have configured Vault as a System Service and created a vault.service file, Created the vault configuration, data & logs directory. Also changed the ownership of vault directory to vault user, Created a vault.hcl file which holds all the vault configuration.
Then I have start the vault service and checked the status of vault service but it gave error of "service exited", then i have explored and tried another blog and it running fine, Hit Public IP on browser with port number 8200, and vault ui interface open, when access the vault UI, by default it will be sealed and Initialised vault using initialise button with 3 key shares. and Download the keys and it also contain vault token,I have Entered three keys one by one from the downloaded key file to unseal vault and once unsealed, login to vault with the root_token.
After created vault cluster, i have tried to install vault agents on EKS,i have generated a token and export into the Cluster as well as export endpoint of vault cluster. then added hashicorp helm repo using "helm repo add" command and created values.yaml file and inject	vault cluster endpoint in values.yaml file. then install vault using helm install command and passed values.yaml file with helm command.
Then I have configured kubernetes auth method on HCP Vault using vault auth enable command, after configured authentication method of vault cluster, created own roles for our application and created a policy and gave read permission.
After that i have enabled secrets using this command & put secret into file and passed MONGO_URL variable, defined vault annotation in deployment file of catalogue then I have applied deployment file of microservice but vault container not up it gave error of authentication error checked logs with vault-init.
I have Explored for authentication error, then I have known In HCP cloud vault has namespace and namespace has locked the token and it authenticate the vault agent. but it installed on sever it ask to choose enterprise version for using namespace.
Then i have tried to commented out with this annotation "vault.hashicorp.com/namespace: "admin" but error not resloved.and explored for that and tried with token authentication "vault.hashicorp.com/auth-path: "auth/token" and also tried with many solution but error got not resolved. then i have tried to pass token name and vault endpoint as env variable in aplication deployment file but error still persist and currently working on that. 

Firstly I have Attended cost analysis webinar one hour, then i have worked on pervious error "authentication error" of vault. i have tried to commented out with this annotation "vault.hashicorp.com/namespace: "admin" but error not resloved. then i have explored for that, i have found a blog how install standalone vault on kubernetes cluster & checked with deployment file of and used this annotation "agent-inject-secret-catalogue.sh" and tried with error got resloved.
After that, I have worked on integrated deployment file with helm charts and ran helm install command but it gave error of "secret not defined", explored for that but error still persist.
Had meeting with nitin sir, he assigned taks Fix namespace based scheduling, default storage class in azure . price to performance ratio, default encryption, resizing support, tiered subnets ( app and db tier ), prepare for Vault demo ( volume mount , secret creation and configmap ).
Then I have worked on perivous helm error was {{- with secret }} this format of helm when we use {{- with secret }} with vault annotation it gave error "secret not defined" when ran helm install command, then i have tried with {{ `{{ - with secret }}` }} and ran helm install command and error got resoved.
After that I have worked on default storage class in azure, price to performance ratio and explored for that i have found this blog https://azure.microsoft.com/en-in/pricing/details/managed-disks/ and known that standard_LRS storage is very cost reliable and currently working on resizing support on Storage.


Firstly I have explored how to kubernetes External secrets allows to use external secret management systems for HashiCorp Vault to securely add secrets in Kubernetes.I have created the kubernetes-external-secrets resources using helm charts when i have deployed helm chart then checked with pods and pods were not running up and it gave error of configerror. then I have Vault endpoint passed into the values.yaml file and ran helm upgrade command and pods were running up.
Then i have craeted yaml file for ExternalSecret and applied that and it created ExternalSecret successfully,checked with "kubectl get externalsecret" command this show it tried to connect with 127.0.0.1:8200 intead of custom endpoint of vault.i have explored for that and tried with change vaultMountpoint but it not resloved and also tried with many solution but error still persist as posted into the group.
After that, I have tried kubernetes-external-secrets supports fetching secrets from Azure Key vault. first i have craeted an aks cluster in azure, then i have tried to configured and run the Azure Key Vault provider for the Secrets Store CSI driver on aks, install Secrets Store CSI driver using helm chart on aks, then i have created created a azure key vault using thhis command "az keyvault create" and set secret using this command "az keyvault secret set".
Then I have created own custom SecretProviderClass object with provider-specific parameters for the Secrets Store CSI driver & created a yaml for that with using these parameters "keyvault","objects", "tenanatID" and applied that.
After that I have required role assignments with Azure Active Directory pod identity, when i have tried to create role it gave error of permission. I will require the access of "user access administrator" on subscription level it require this access for creating roles in azure.
I have worked on resizing of pods, used this annotation "allowVolumeExpansion: true" in storageclass yaml and applied that. then I have installed jenkins  service with 8gb pvc and then i have changed the size of pvc 20gb in values.yaml file and applied that & checked pvc size, size incresed 8gb to 20gb.now currently working on configured the Azure Key Vault provider for the Secrets Store CSI driver on Kubernetes. 

Firstly, I have created aks cluster in azure using azure cli and created a azure key vault, need to created role for that but it requried "user access administrator" on subscription level, asked to nitin sir. 
Then i have worked on develop Azure automation at par with standards used in Aws Automation, i have created Postgres database using terraform code in the azure and also checked with SPOT nodepool in the azure automation.
After that I have worked on ISTIO implementation in sample microservice application,first I have explored about that and add binary for the istioctl then i have install istio using this command "istioctl install --set profile=demo -y", then I have deployed sample microservice application in different namespace and configured istio ingress by applying gateway.yaml file. but it gave error of "no matches for kind "Gateway" in version" explore for that and tried to resolved that, now the exposed Istio gateway to access Robot Shop using this command "kubectl -n istio-system get svc istio-ingressgateway" and it provide the LoadBalancer endpoint of the Istio gateway and hit on browser and it work fine and for view dashboard of istio i have explored and  tried to install kiali but i have not shown the dashboard of istio services and currently working on that.


Firstly,Had meeting with anoushka and she told about creation of Certmanager error in terraform when ran terraform apply with plan.out file, then she shared terraform code with me and asked to check, I have checked with code and added dependecies of kubeconfig in certmanager resource of terraform and ran terraform apply & error got resolved.
After that I have created aks cluster and tried to configured and run the Azure Key Vault provider for the Secrets Store CSI driver on aks, install Secrets Store CSI driver using helm chart on aks, then i have created created a azure key vault using thhis command "az keyvault create" and set secret using this command "az keyvault secret set".
Then I have created own custom SecretProviderClass object with provider-specific parameters for the Secrets Store CSI driver & created a yaml for that with using these parameters "keyvault","objects", "tenanatID" and applied that. then I have assigned specific roles to the AKS cluster.
Then I have Installed the Azure Active Directory (Azure AD) identity into AKS using helm chart but aad-pod-identity-nmi gave error with azure kubenet networking then i have passed --set nmi.allowNetworkPluginKubenet=true with helm chart and running fine, created an User-Assigned managed identity, then grant the identity permissions to get secrets from your key vault, Used the clientId from the User-assigned managed identity.
After that for deploying pod with mounted secrets from your key vault created podIdentityAndBinding and podBindingDeployment then checked pod status and secret content but it gave error "MountVolume.SetUp failed for volume "secrets-store-inline" : rpc error: code = Unknown desc" and tried to resolved that and tried with many solution but error still persist and currently working on that.

Firstly, I have worked on pervious error "MountVolume.SetUp failed for volume "secrets-store-inline" : rpc error: code = Unknown desc",deployed Aks cluster and tried to integrate azure keyvault with aks but it gave same error,i Explored for that and assigned the reader role to the azure keyvault and error got resloved.
Then I have created the kubernetes-external-secrets resources using helm charts when i have deployed helm chart then checked with pods and pods were not running up and it gave error of configerror. then i have created secret in cluster for saving azure credentials and these credentials inro values.yaml file of helm-charts and again deployed helm chart and error got resolved.
I have created ExternalSecret yaml file, defined backend as azure keyvault & applied that file and ExternalSecret created but it gave error of "does not have secrets get permission on key vault", then i have explored for that i have assigned permission on azure keyvault. and again applied ExternalSecret file then it gave error of "to use on-behalf-of (OBO) flow", explored for that and i have assigned permission of get to appID which required and error got resloved and ExternalSecret created, then i have defined secrets in deployemnt file of one microservice and exec into the pod and secret mount in the pods.
   
Firstly, I have created mysqldb in azure using this command "az mysql server create" in azure cli, then i have tried to connect with virtual machine, defined public ip of machine in mysql-server in connection string and it got connected. 
Then i have explored that how to connect with all virtual machine in same vnet and how to connect with aks cluster, then i have created a service endpoint of Microsoft.SQL for the subnet and update endpoints in mysql-server connection string option and tried to connect with virtual machine and it got connected & also checked with aks node it work fine.
After That I have Explored how to created mysql-db using terraform automation.I have created mysql-server using terraform module, it gave erorr of the "network.SubnetsClient#CreateOrUpdate: Failure sending request:" and ran again terraform apply it gave error of this "do not have ServiceEndpoints for Microsoft.Sql resources configured", I have explored for that and again upadte the service endpoint of Microsoft.SQL for subnet and ran again terraform apply it works fine it created a mysql-server, then i have tried to connect mysql with vm it gave error of "Access denied" i have tried many to resolved error but error still persist.
I have used terraform resources instead of module and created mysql-server, tried to connect with virtual machine it got connected, then i have deployed again mysql-server with terraform module and tried to connect with virtual machine but it gave error of "Access denied" and tried with many solution but error still persist currently working on that. 

AKIAWEGFB4YZN3RK2V4R
jB+Bh84SHYCaoHDmH7ME66ms/z8TwhT0rf/jAgUY

Firstly, I have tested aws terraform automation code and push code into aws-> dev branch.
Then I have checked with nodes memory and cpu usages, checked cluster-autoscaler logs in loki dashboard it 98% or 68 % uses memory but In rancher Cluster Explorer Option it show wrong info about that. and also checked with rancher manger it show full utiligation of cpu and memory.
Update the Excel Sheet for the AWS Node Group Instance type and its CPU and Memory.
After that, I have worked on azure terraform automation and Deployed aks cluster using terraform module and tested that it works fine.







